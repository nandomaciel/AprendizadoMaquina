{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dia 3\n",
    "\n",
    "Tópicos que serão abordados: \n",
    "- Redes Neurais\n",
    "    - Perceptrons\n",
    "    - Multilayer Perceptrons\n",
    "    \n",
    "**Documentação:**<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html <br>\n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "Para certos tipos de problemas, redes neurais estão entre os métodos mais efetivos e robustos de aprendizado conhecidos hoje. Os algoritmos supervisionados baseados em redes neurais no scikit-learn, que iremos cobrir hoje, usam um algoritmo de *backpropagation* que já foi verificado surpreendentemente bem-sucedido em diversas aplicações práticas, como reconhecimento de caracteres escritos à mão, de palavras faladas, e de faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "O Perceptron é uma rede neural artificial criada por [Frank Rosenblatt](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm), tida como a rede neural mais simples, sendo um classificador linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import perceptron\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurando os dados para a classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame({\n",
    "'x' : [2, 1, 2, 5, 7, 2, 3, 6, 1, 2, 5, 4, 6, 5],\n",
    "'y' : [2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7],\n",
    "'Targets' : [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seta um arranjo de cores\n",
    "colormap = np.array(['r', 'k'])\n",
    " \n",
    "# Plotar os dados em seus respectivos eixos\n",
    "# Configura o arranjo de cores para os Targets\n",
    "plt.scatter(inputs.x, inputs.y, c=colormap[inputs.Targets], s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria objeto Perceptron\n",
    "net = perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002)\n",
    " \n",
    "# Treina objeto Perceptron\n",
    "net.fit(inputs[['x', 'y']], inputs['Targets'])\n",
    "\n",
    "# Print the results\n",
    "pred = net.predict(inputs[['x', 'y']])\n",
    "print(\"Prediction\", pred)\n",
    "print(\"Actual\", np.array(inputs.Targets))\n",
    "print(\"Training accuracy \", net.score(inputs[['x', 'y']], inputs[['Targets']]) * 100, \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data\n",
    "plt.scatter(inputs.x, inputs.y, c=colormap[inputs.Targets], s=40)\n",
    " \n",
    "# Calc the hyperplane (decision boundary)\n",
    "ymin, ymax = plt.ylim()\n",
    "w = net.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(ymin, ymax)\n",
    "yy = a * xx - (net.intercept_[0]) / w[1]\n",
    " \n",
    "# Plot the hyperplane\n",
    "plt.plot(xx, yy, 'k-')\n",
    "plt.ylim([0,8]) # Set the y axis size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1:\n",
    "\n",
    "Utilize o seguinte conjunto de dados para demonstrar que um Perceptron é capaz de \"executar\" uma porta lógica AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame({\n",
    "'A' : [0, 0, 1, 1],\n",
    "'B' : [0, 1, 0, 1],\n",
    "'Targets' : [0, 0, 0, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array(['r', 'k'])\n",
    "plt.scatter(inputs.A, inputs.B, c=colormap[inputs.Targets], s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2:\n",
    "\n",
    "Repita o processo para uma porta OR:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame({\n",
    "'A' : [0, 0, 1, 1],\n",
    "'B' : [0, 1, 0, 1],\n",
    "'Targets' : [0, 1, 1, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Uma porta XOR\n",
    "\n",
    "É possível implementar uma porta XOR com apenas um perceptron? Teste!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame({\n",
    "'A' : [0, 0, 1, 1],\n",
    "'B' : [0, 1, 0, 1],\n",
    "'Targets' : [0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "colormap = np.array(['r', 'k'])\n",
    "plt.scatter(inputs.A, inputs.B, c=colormap[inputs.Targets], s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame({\n",
    "'A' :       [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5],\n",
    "'B' :       [1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4],\n",
    "'Targets' : [0,0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,1,1,1,1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnóstico de câncer de mama\n",
    "Usaremos uma base de dados pré-carregada do scikit-learn (cópia desta: https://goo.gl/U2Uwz2). Nela, cada uma das 569 instâncias contém 30 atributos númericos coletados de uma amostra de tecido, que serão usados para determinar a natureza de um nódulo no tecido mamário (0 - maligno ou 1 - benigno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após importar os dados, e separá-los entre um conjunto de treino e um de teste, temos que lidar agora com uma das desvantagens do tipo de modelos de redes neurais com as quais iremos trabalhar (*Multi-layer Perceptron* ou MLP): MLPs são sensíveis a escala dos atributos. Isto quer dizer que teremos que \"normalizar\" os valores dos atributos como parte do pré-processamento dos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instanciamos o scaler do sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit computa a média e o desvio padrão que serão usados\n",
    "# para a normalização dos dados, a partir dos dados de treinamento\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Agora transformamos de fato todos os dados\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos instanciar um objeto MLPClassifier, já que o problema é de classificação, treiná-lo com nossos dados, e testar suas previsões. Neste momento, teremos que lidar com outra dificuldade desses modelos: o ajuste dos seus diferentes hiperparâmetros, como número de neurons ocultos, camadas, e número máximo de iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Por padrão, por exemplo, temos uma única camada oculta com 100 neurons\n",
    "mlpc = MLPClassifier()\n",
    "print(mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc.fit(X_train, y_train)\n",
    "score = mlpc.score(X_test, y_test)\n",
    "\n",
    "print(score * 100, '%', sep='')\n",
    "print((1-score) * y_test.shape[0], 'erros de diagnóstico')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos observar uma desvantagem particular de redes neurais: os pesos que a rede adquire após o treinamento são geralmente de difícil interpretação. O comportamento de rede neurais não é tão facilmente interpretado como, por exemplo, o comportamento de uma árvore de decisão.\n",
    "\n",
    "Outra observação importante é que as funções de perda de MLPs tem mais de um mínimo local. Portanto dependendo de como os pesos são inicializados, teremos uma acurácia diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo apontado essas desvantagens de MLPs e de outros tipos de redes neurais, eles ainda são métodos de aprendizado extremamente úteis por sua capacidade de aprender modelos não-lineares, de aprender modelos em tempo real a medida que novos dados são disponibilizados e por sua robustez, isso é, boa performance com dados ruidosos e inexatos. <br>\n",
    "De fato, quando mencionamos acima que redes neurais são especialmente bem-sucedidas com certos tipos de problemas, queremos dizer problemas nos quais os dados de treinamento são complexos, ruidosos, e inexatos, como aqueles obtidos de sensores, câmeras, e microfones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconhecimento de dígitos\n",
    "Faremos mais um exemplo de classificação, dessa vez para a identificação de dígitos escritos à mão. Usando a função load_digits de sklearn.datasets, obtemos um conjunto de 1797 imagens *grayscale* de 8x8 pixels (images), e as mesmas imagens no formato de arrays de tamanho 64 (data) que podemos usar para treinar e testar um novo modelo. Dessa forma, cada atributo é a luminosidade de um determinado pixel da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "\n",
    "img = 1400\n",
    "print(digits.data[img])\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[img])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente agora processar os dados, treinar e testar um MLPClassifier. Tente também analisar seus resultados. Como alterar os hiperparâmetros afeta esses resultados? E o tempo de execução? Há grande variação no modelo resultante entre uma execução e outra? <br>\n",
    "Note que não estamos trabalhando com grandes quantidades de dados. Em aplicações práticas testar um única idéia pode custar horas de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximação de uma função\n",
    "Agora veremos um exemplo que usa um MLP para regressão. Vamos aproximar a simples função quadrática y=x<sup>2</sup> a partir de dados ruidosos sobre a função. Nesse caso sabemos a função que queremos aproximar, mas em aplicações reais teríamos apenas os pontos de dados coletados e tentaríamos aproximar uma função com eles. <br>\n",
    "Observe o exemplo abaixo. Como estão os resultados do nosso modelo? Podemos fazer alguma mudança para melhorá-los?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "%matplotlib inline\n",
    "\n",
    "#np.random.seed(3)\n",
    "# Primeiramente geramos dados aleatórios:\n",
    "interval = 4\n",
    "n = 20\n",
    "e = 2\n",
    "# aqui geramos n pontos aleatórios em (-interval, interval) \n",
    "X_train = np.random.uniform(-interval, interval, size = n)\n",
    "# e para cada um deles damos um y = x^2 mais um erro ou ruído\n",
    "# que teríamos ao obter esses dados\n",
    "y_train = X_train**2 + e*np.random.randn(n, )\n",
    "\n",
    "# Então vamos transformar nossos dados em nparrays com as\n",
    "# dimenções certas para treinar o objeto MLPRegressor\n",
    "X_train = np.reshape(X_train, [n, 1])\n",
    "y_train = np.reshape(y_train, [n ,])\n",
    "\n",
    "# Agora visualizamos esses pontos\n",
    "plt.scatter(X_train, y_train, color='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos o MLPRegressor e o treinamos\n",
    "mlpr = MLPRegressor(alpha=0.00001, hidden_layer_sizes = (2,5,2), max_iter = 50000, \n",
    "                 activation = 'tanh')\n",
    "mlpr.fit(X_train, y_train)\n",
    "\n",
    "# Agora vamos testá-lo\n",
    "test_density = 100\n",
    "# fazemos um arranjo com test_density pontos uniformemente\n",
    "# espaçados no intervalo (-interval, interval)\n",
    "X_test = np.linspace(-interval, interval, test_density)\n",
    "# e mudamos também suas dimensões para passá-lo pro nosso preditor\n",
    "X_test = np.reshape(X_test, [test_density, 1])\n",
    "\n",
    "# Finalmente usamos nosso modelo para estimar uma função e a visualizamos\n",
    "y_pred = mlpr.predict(X_test)\n",
    "plt.scatter(X_train, y_train, color='c')\n",
    "plt.plot(X_test, X_test**2, color = 'k', ls='--')\n",
    "plt.plot(X_test, y_pred, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ainda não o fez, tente agora alterar alguns dos hiperparâmetros mais importantes de MLPClassifier e MLPRgressor listados abaixo, e analize seus resultados.\n",
    "- **alpha** é um parâmetro de regularização. Quanto maior, maior será a penalidade por pesos grandes no aprendizado da rede. default=0.0001\n",
    "- **hidden_layer_sizes** é uma tupla definindo o número de layers ocultas no seu MLP e o tamanho de cada uma. default=(100,)\n",
    "- **max_iter** define o número máximo de iterações de aprendizado, se o algoritmo não convergir antes disso (terminar por falta de melhoria entre uma iteração e outra). default=200\n",
    "- **solver** define o algoritmo usado para otimização dos pesos, uma escolha dentre {'lbfgs', 'sgd', 'adam'}. 'adam' geralmente funciona bem para datasets grandes (milhares de amostras). 'lbfgs' pode ser melhor em performance e tempo de convergência para datasets menores. Mas 'sgd' pode superar os outros dois sob certas condições, contato que se escolha bem o learning_rate. default='adam'\n",
    "- **learning_rate** define a \"velocidade\" de aprendizado do seu modelo, podendo escolher entre 'constant', que mantém a taxa dada pelo parâmetro **learning_rate_init** (default=0.001); 'invscaling', que gradualmente diminui essa taxa; e 'adaptive' que mantêm a taxa enquanto o modelo está melhorando mas a diminui quando ele \"estagna\". default='constant'\n",
    "- **activation** define a função de ativação dos Perceptrons dentre {'identity', 'logistic', 'tanh', 'relu'}, sobre as quais não entraremos em detalhes. default='relu' <br>\n",
    "Veja mais sobre na documentação no topo da página."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
